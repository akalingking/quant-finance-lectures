{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16a6952",
   "metadata": {},
   "source": [
    "# Hamilton-Jacobi-Bellman Equation\n",
    "\n",
    "## Dynamic Programming in Continuous Time\n",
    "\n",
    "Consider a continuous optimal control problem described by the state equation:\n",
    "\\begin{align}\n",
    "\\dot{x}=f\\big(x(t), u(t), t\\big) \\qquad (1.1)\n",
    "\\end{align}\n",
    "\n",
    "To be controlled by minimizing the performance measure\n",
    "\\begin{align}\n",
    "J &=h\\big(x(t_f), t_f\\big) + \\int_{t_0}^{t_f}g\\big(x(\\tau), u(\\tau), \\tau\\big) d\\tau \\qquad (1.2)\\\\\n",
    "\\textit{where:}&\\\\\n",
    "g,h &\\text{ are scalar functions}\\\\\n",
    "t_0, t_f &\\text{ are fixed time intervals}\\\\\n",
    "\\tau &\\text{ is integration parameter}\\\\\n",
    "u(t) \\in U & \\text{ set of constraint/utility functions}\n",
    "\\end{align}\n",
    "\n",
    "The goal is to pick $u(\\tau)$, $t \\leq \\tau \\leq t_f$ to minimize (1.2).\n",
    "\n",
    "### General Solution Steps\n",
    "- Split the time interval $[t,t_f]$ into $[t, (t+\\triangledown t)]$ and $[(t+\\triangledown t), t_f]$ where ideally $\\triangledown t \\rightarrow 0$.\n",
    "- Identify the optimal cost-to-go $J^*(x(t+\\triangledown t),(t+\\triangledown t)$.\n",
    "- Determine the \"stage cost\" in time $[t, (t+\\triangledown t)]$.\n",
    "- Find the best strategy from time $t \\rightarrow (t+\\triangledown t)$.\n",
    "- Manipulate result into HJB Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d298d806",
   "metadata": {},
   "source": [
    "## Derivation of HJB Equation\n",
    "The continuous optimal control problem will be included in a larger class of problems by considering the performance measure\n",
    "\\begin{align}\n",
    "J\\big(x(t), {u(\\tau)}_{t\\leq\\tau\\leq T}, t\\big) = h\\big(x(t_f), t_f\\big) + \\int_{t}^{t_f} g\\big(x(\\tau), u(\\tau), \\tau\\big)d\\tau \\qquad (1.3)\n",
    "\\end{align}\n",
    "\n",
    "where $t$ can be any value less than or equal to $t_f$ and $x(t)$ can be any admissible state value. Notice that the performance measure will depend on the numerical values for $x(t)$ and $t$ on the optimal control history in the interval $[t, t_f]$ (Kirk, 2004).\n",
    "\n",
    "Let us now attempt to determine the control that minimize (1.3) for all admissible $x(t)$ and all $t \\leq t_f$. The minimum cost function is then:\n",
    "\n",
    "\\begin{align}\n",
    "J^*\\big(x(t), t\\big) = \\min_{u(\\tau)} \\left\\{ h\\big(x(t_f), t_f\\big) + \\int_{t}^{t_f}g\\big(x(\\tau), u(\\tau), \\tau\\big) d\\tau \\right\\} \\qquad (1.4)\n",
    "\\end{align}\n",
    "\n",
    "We shall split the time interval $[t, t_f]$ into $[t, (t+\\triangledown t)]$ and $[(t+\\triangledown t), t_f]$ and we are specifically interested where $\\triangledown t \\rightarrow 0$. By subdividing the time interval, we obtain:\n",
    "\n",
    "\\begin{align}\n",
    "J^*(x(t),t)=\\min_{u(\\tau)} \\left\\{ h(x(t_f), t_f) + \\int_{t}^{t+\\triangledown t}g(x,u,\\tau)d\\tau + \\int_{t+\\triangledown t}^{t_f} g(x,u,\\tau)d\\tau\\right\\} \\qquad (1.5)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1c7db1",
   "metadata": {},
   "source": [
    "The principle of optimality requires that\n",
    "\\begin{align}\n",
    "J^*(x(t),t)=\\min_u \\left\\{ \\int_{t}^{t+\\triangledown t} g(x,u,\\tau)d\\tau + J^*(x(t+\\triangledown t), t+\\triangledown t) \\right\\} \\qquad (1.6)\n",
    "\\end{align}\n",
    "\n",
    "where $J^*\\big(x(x+\\triangledown t), x+\\triangledown t\\big)$ is the minimum cost of the process for the time interval $(t+\\triangledown t) \\leq \\tau \\leq t_f$ with initial state $x(t+\\triangledown t)$.\n",
    "\n",
    "Assuming $J^*$ has bounded second derivatives in both arguments, we can expand $J^*(x(t+\\triangledown t), t+\\triangledown t)$ in a Taylor series about the point $(x(t), t)$ to obtain:\n",
    "\n",
    "\\begin{align}\n",
    "J^*\\big(x(t),t\\big) &= \\min_u \\left\\{ \\int_{t}^{t+\\triangledown t} g(x,u,\\tau)d\\tau + J^*(x(t), t) + \\left[\\frac{\\partial J^*}{\\partial t}(x(t),t)\\right] \\triangledown t \\\\+ \\left[\\frac{\\partial J^*}{\\partial x}(x(t),t)\\right]^T [x(t+\\triangledown t) - x(t)] \\right\\} \\qquad (1.7)\n",
    "\\end{align}\n",
    "\n",
    "For small $\\triangledown t$:\n",
    "\\begin{align}\n",
    "x(t+\\triangledown t) - x(t) &= x(t) \\cdot \\triangledown t\\\\\n",
    "\\\\\n",
    "\\int_{t}^{t+\\triangledown t} g(x, u, \\tau)d\\tau &= g(x,u,t)\\triangledown t\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31512f81",
   "metadata": {},
   "source": [
    "If $J^*(x(t),t)$ is independent of $u$, we can cancel the terms from the right and left hand sice of (1.7). $\\dot{x}$ will be replaced using the state equation (1.1).\n",
    "\n",
    "\\begin{align}\n",
    "0=\\min_{u(\\tau)} \\left\\{ g(x,u,t)\\triangledown t + \\frac{\\partial J^*(x(t),t)}{\\partial t} \\triangledown t + \\left[\\frac{\\partial J^*(x(t),t) }{\\partial x} \\right]^Tf(x,u,t)\\triangledown t \\right\\} \\qquad (1.8)\n",
    "\\end{align}\n",
    "\n",
    "Dividing the above equation by $\\triangledown t$ yields:\n",
    "\n",
    "\\begin{align}\n",
    "0=\\frac{\\partial J^*(x(t),t)}{\\partial t} + \\min_{u(\\tau)} \\left\\{ g(x,u,t) + \\left[\\frac{\\partial J^*(x(t),t) }{\\partial x} \\right]^Tf(x,u,t) \\right\\} \\qquad (1.9)\n",
    "\\end{align}\n",
    "\n",
    "Define the Hamiltonian as\n",
    "\n",
    "\\begin{align}\n",
    "H=g(x,u, \\tau) + \\left[\\frac{\\partial J^*(x(t),t)}{\\partial x}\\right]^T f(x, u, t) \\qquad (1.10)\n",
    "\\end{align}\n",
    "\n",
    "Then we write the partial differential equation of (1.9) as\n",
    "\n",
    "\\begin{align}\n",
    "0=\\frac{\\partial J^*(x(t),t)}{\\partial t} + \\min_u H \\qquad (1.11)\n",
    "\\end{align}\n",
    "\n",
    "To find the boundary value for the differential equation set $t=t_f$ from (1.4) we have:\n",
    "\n",
    "\\begin{align}\n",
    "J^*(x(t_f),t_f)=h(x(t_f), t_f) \\qquad (1.12)\n",
    "\\end{align}\n",
    "\n",
    "We have obtained the HJB equation (1.11) subject to boundary conditions (1.12). It provides the solution to the optimal control problems for general non-linear dynamical systems. However, analytical solution to the HJB Equation is difficult to obtain in most cases. A soluition can be obtained analitically by guessing a form of the minimum cost function. In general, the HJB equation must be solved by numerical techniques. Actually, a numerical solution involves some sort of a discrete approximation to the exact optimization relationship (1.11)(Kirk, 2004)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4db7a9",
   "metadata": {},
   "source": [
    "**Example 1**. A first order system is described by the differential equation\n",
    "\n",
    "\\begin{align}\n",
    "\\dot{x}(t) = x(t) + u(t)\n",
    "\\end{align}\n",
    "\n",
    "It is desired to find the control law that minimizes the cost function\n",
    "\n",
    "\\begin{align}\n",
    "J=\\frac{1}{4}x^2(T) + \\int_{0}^{T}\\frac{1}{4}u^2(t)dt\n",
    "\\end{align}\n",
    "\n",
    "The final time $T$ is specified and the admissible state and control values are constrained by any boundaries\n",
    "\n",
    "\\begin{align}\n",
    "g\\big(x(t), u(t), t\\big) = \\frac{1}{4}u^2(t) \\\\\n",
    "f\\big(x(t), u(t), t\\big) = x(t) + u(t)\n",
    "\\end{align}\n",
    "\n",
    "The Hamiltonian\n",
    "\n",
    "\\begin{align}\n",
    "H\\left(x(t), u(t), \\frac{\\partial J^*}{\\partial x}\\right)=\\frac{1}{4}u^2t+\\frac{\\partial J^*(x(t), t)}{\\partial x} (x(t) + u(t))\n",
    "\\end{align}\n",
    "\n",
    "Since the control is unconstrained, the necessary condition that the optimal control must satisfy is\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial H}{\\partial u} = \\frac{1}{2}u(t) + \\frac{\\partial J^*}{\\partial x} = 0\n",
    "\\end{align}\n",
    "\n",
    "The control indeed minimizes the Hamiltonian function because\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial^2 H}{\\partial u^2} =  \\frac{1}{2} \\gt 0\n",
    "\\end{align}\n",
    "\n",
    "The optimal control\n",
    "\\begin{align}\n",
    "u^*(t) = -2\\frac{\\partial J^*}{\\partial x}\n",
    "\\end{align}\n",
    "\n",
    "When substituted into the HJB equation\n",
    "\n",
    "\\begin{align}\n",
    "0=\\frac{\\partial J^*}{\\partial t} + \\min_u H\n",
    "\\end{align}\n",
    "\n",
    "Gives\n",
    "\n",
    "\\begin{align}\n",
    "0&=\\frac{\\partial J^*}{\\partial t} + \\frac{1}{4}\\left(-2\\frac{\\partial J^*}{\\partial x}\\right)^2 + \\frac{\\partial J^*}{\\partial x}\\left(x(t) - 2\\frac{\\partial J^*}{\\partial x}\\right)\\\\\n",
    "\\\\\n",
    "0&=\\frac{\\partial J^*}{\\partial t} - \\frac{\\partial J^*}{\\partial x} + \\frac{\\partial J^*}{\\partial x}x(t) \\qquad (1.13)\n",
    "\\end{align}\n",
    "\n",
    "The boundary value of $J^*$ is\n",
    "\n",
    "\\begin{align}\n",
    "J^*(x(T), T) = \\frac{1}{4}x^2(T) \\qquad (1.14)\n",
    "\\end{align}\n",
    "\n",
    "One way to solve the HJB equation is to guess a form of the solution and see if it can be made to satisfy the differential equation  and the boundary conditions. Since $J^*(x(T), T)$ is quadratic in $x(T)$ guess:\n",
    "\\begin{align}\n",
    "J^*(x(t), t) = \\frac{1}{2}p(t)x^2(t) \\qquad (1.15)\n",
    "\\end{align}\n",
    "\n",
    "where $p(t)$ represents the unknown scalar function of $t$ that is to be determined, notice that\n",
    "\\begin{align}\n",
    "\\frac{J^*}{\\partial x}=p(t)x(t)\n",
    "\\end{align}\n",
    "\n",
    "which together witht the expression determined for $u^*(t)$ implies that\n",
    "\\begin{align}\n",
    "u^*(t) = -2p(t)x(t)\n",
    "\\end{align}\n",
    "\n",
    "Thus if $p(t)$ can be found such that (1.13) and (1.14) are satisfied, the optimal control is a linear feedback of the state-indeed this was the motivation for selection the form (1.15)\n",
    "\n",
    "Substituting (1.15) and \n",
    "\\begin{align}\n",
    "\\frac{\\partial J^*}{\\partial t} = \\frac{1}{2}p(t)x^2(t)\n",
    "\\end{align}\n",
    "\n",
    "into (1.13) gives\n",
    "\\begin{align}\n",
    "0=\\frac{1}{2}p(t)x^2t - p^2(t)x^2(t) + p(t)x^2(t)\n",
    "\\end{align}\n",
    "\n",
    "Since this equation must be satisfied for all $x(t)$, $p(t)$ must be calculated from the differential equation\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{2}p(t)-p^2(t) + p(t) = 0 \\qquad (1.18)\n",
    "\\end{align}\n",
    "\n",
    "With the final condition $p(T)=\\frac{1}{2}$. $p(t)$ is a scalar function of $t$, therefore the solution can be obtained using the transformation $z(t)=1/p(t)$ with the result:\n",
    "\n",
    "\\begin{align}\n",
    "p(t)=\\frac{1}{1+e^{2(t-T)}}\n",
    "\\end{align}\n",
    "\n",
    "The solution of 1.18 is obtained as follows\n",
    "\n",
    "\\begin{align}\n",
    "z(t)=\\frac{1}{p(t)}; z(T)=\\frac{1}{p(T)}=2\\\\\n",
    "(1.18) \\Rightarrow -\\frac{\\dot{z}}{z^2}-\\frac{2}{z^2} + \\frac{2}{z}=0\\\\\n",
    "\\dot{z} - 2z + 2 = 0\n",
    "\\end{align}\n",
    "\n",
    "The solution of the homogenous part of the above equation is:\n",
    "\n",
    "\\begin{align}\n",
    "\\dot{z} -2z = 0, is z(t)=C_1e^{2t}\n",
    "\\end{align}\n",
    "\n",
    "and the general solution\n",
    "\n",
    "\\begin{align}\n",
    "z(t) = C_1e^{2t} + C_2\n",
    "\\end{align}\n",
    "\n",
    "The constants are calculated by replacing the general solution into the equation and using the final condition\n",
    "\n",
    "\\begin{align}\n",
    "2C_1e^{2t} - 2(C_1e^{2t} + C_2) + 2 &= 0 \\\\\n",
    "-2C_2+2 &= 0 \\\\\n",
    "C_2 &= 1\\\\\n",
    "\\\\\n",
    "z(t) &= C_1e^{2t} + 1\\\\\n",
    "z(T) &= C_1e^{2T}+1 = 2\\\\\n",
    "C_1 &= e^{-2t}\n",
    "\\end{align}\n",
    "\n",
    "Then the general solution yields\n",
    "\n",
    "\\begin{align}\n",
    "z(t)=e^{2(t-T)}+1 \\Rightarrow p(t)=\\frac{1}{e^{2(t-T)}+1}\n",
    "\\end{align}\n",
    "\n",
    "The optimal control law is then\n",
    "\n",
    "\\begin{align}\n",
    "u^*(t)=-2\\frac{\\partial J^*(x,t)}{\\partial x}=-2p(t)x(t)\\\\\n",
    "u^*(t)=- \\frac{2}{1+e^{2(t-T)}}x(t)\n",
    "\\end{align}\n",
    "\n",
    "![Optimal Control Diagram](images/optimal_control_sample_diagram.jpg)\n",
    "\n",
    "Notice that as $T\\rightarrow \\infty$, the linear time varying feedback approaches constant feedback $p(t)$ and controlled system\n",
    "\n",
    "\\begin{align}\n",
    "\\dot{x}(t) &= x(t)-2x(t)\\\\\n",
    "&= -x(t)\n",
    "\\end{align}\n",
    "is stable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94a9b4",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] [Hamilton-Jacobi-Bellman Equation, wiki](https://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi%E2%80%93Bellman_equation)\n",
    "\n",
    "[2] [Optimal Control Theory. D. Kirk](https://www.amazon.com/Optimal-Control-Theory-Introduction-Engineering/dp/0486434842)\n",
    "\n",
    "[3] [Dynamic Programming, Richard Bellman, Dover](https://books.google.com.ph/books/about/Dynamic_Programming.html?id=CG7CAgAAQBAJ&redir_esc=y)\n",
    "\n",
    "[4] [Dynamic Programming and Optimal Control, Vol 1 & 2, D.P. Bertsekas](https://www.mit.edu/~dimitrib/dpbook.html)\n",
    "\n",
    "[5] [Applied Optimal Control. Bryson & Ho](https://books.google.com.ph/books/about/Applied_Optimal_Control.html?id=P4TKxn7qW5kC&redir_esc=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hummingbot)",
   "language": "python",
   "name": "hummingbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
